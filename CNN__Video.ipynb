{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_ Video.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/notmanan/Depression-Detection-Through-Multi-Modal-Data/blob/master/CNN__Video.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncPer7MulwaY",
        "colab_type": "text"
      },
      "source": [
        "**Connecting Drive to Google Colab**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FkprWXdn7N3",
        "colab_type": "code",
        "outputId": "2257e0a7-95cd-4e55-d0c3-deadc9457154",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras import metrics\n",
        "from sklearn.metrics import classification_report\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv1D\n",
        "from keras.layers import MaxPooling1D\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dense"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dzxb5JsroQ5y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def processData(data):\n",
        "    X = data.iloc[:,:].values\n",
        "    X = np.delete(X, 0, 1)\n",
        "    X = np.delete(X, 1, 1)\n",
        "    for i in range(len(X)):\n",
        "        if(isinstance(X[i][5],str) or isinstance(X[i][7],str)):\n",
        "            X[i] = np.zeros((1, X.shape[1]))\n",
        "            # print(\"se\" , end = \" \")\n",
        "    return X\n",
        "\n",
        "def getData(filename):\n",
        "    data = pd.read_csv(filename,delimiter=',', engine='python')\n",
        "    X = processData(data)\n",
        "    return X\n",
        "\n",
        "def makeDataPoint(patientNo):\n",
        "    file1 = (patientNo)+\"_CLNF_AUs.txt\"\n",
        "    file2 = (patientNo)+\"_CLNF_features.txt\"\n",
        "    file3 = (patientNo)+\"_CLNF_features3D.txt\"\n",
        "    file4 = (patientNo)+\"_CLNF_gaze.txt\"\n",
        "    file5 = (patientNo)+\"_CLNF_hog.txt\"\n",
        "    file6 = (patientNo)+\"_CLNF_pose.txt\"\n",
        "\n",
        "    X1 = getData(file1)\n",
        "    X2 = getData(file2)\n",
        "    X3 = getData(file3)\n",
        "    X4 = getData(file4)\n",
        "    X6 = getData(file6)\n",
        "\n",
        "    X = np.concatenate((X1, X2, X3, X4, X6), 1)\n",
        "    return X\n",
        "\n",
        "def scale_down(X):\n",
        "  X_new = []\n",
        "  size = 2\n",
        "  for i in range(int(X.shape[0]/size)):\n",
        "    cur_row = X[i*size]\n",
        "    for j in range(1,size):\n",
        "      if(i+j < X.shape[0]):\n",
        "        cur_row += X[i+j]\n",
        "    cur_row = cur_row/size\n",
        "    X_new.append(cur_row)\n",
        "  X_new = np.array(X_new)\n",
        "  return X_new\n",
        "\n",
        "def decrease_size(X):\n",
        "  size = 10000\n",
        "  if(X.shape[0] < size):\n",
        "    dif = size - X.shape[0] \n",
        "    temp = np.zeros((dif,X.shape[1]))\n",
        "    X = np.concatenate((X,temp),axis = 0)\n",
        "  elif(X.shape[0] > size):\n",
        "    X = X[:10000, :]\n",
        "  return X\n",
        "\n",
        "def makeDataset(location, folder):\n",
        "    file  = np.array(pd.read_csv(location,delimiter=',',encoding='utf-8'))[:, 0:2]\n",
        "    X_temp = []\n",
        "    Y_temp = []\n",
        "    for i in range(len(file)):\n",
        "      patientID = (str(int(file[i][0])))\n",
        "      string = '/content/drive/My Drive/MCA Dataset/' + folder +\"/\"+ patientID\n",
        "      XT = makeDataPoint(string)\n",
        "      XT = scale_down(XT)\n",
        "      XT = decrease_size(XT)\n",
        "      X_temp.append(XT)\n",
        "      Y_temp.append(int(file[i][1]))\n",
        "    Y_temp = np.asarray(Y_temp)\n",
        "    return X_temp, Y_temp\n",
        "\n",
        "class CNN_Video:\n",
        "  def __init__(self):\n",
        "    # Initialising the CNN\n",
        "    classifier = Sequential()\n",
        "    # Step 1 - Convolution\n",
        "    classifier.add(Conv1D(300, 3, input_shape = (10000, 388), activation = 'relu'))\n",
        "    # Step 2 - Pooling\n",
        "    classifier.add(MaxPooling1D(pool_size = 2))\n",
        "    classifier.add(Conv1D(150, 3, activation = 'relu'))\n",
        "    classifier.add(MaxPooling1D(pool_size = 2))\n",
        "    # Adding a second convolutional layer\n",
        "    classifier.add(Conv1D(75, 3, activation = 'relu'))\n",
        "    classifier.add(MaxPooling1D(pool_size = 2))\n",
        "    classifier.add(Conv1D(32, 3, activation = 'relu'))\n",
        "    classifier.add(MaxPooling1D(pool_size = 2))\n",
        "    # Step 3 - Flattening\n",
        "    classifier.add(Flatten())\n",
        "    # Step 4 - Full connection\n",
        "    classifier.add(Dense(units = 128, activation = 'relu'))\n",
        "    classifier.add(Dense(units = 1, activation = 'sigmoid'))\n",
        "\n",
        "  # Compiling the CNN\n",
        "    classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy')\n",
        "    class_weight = { 0: 0.3, 1:0.7}\n",
        "    self.classifier = classifier\n",
        "\n",
        "  def fitModel(self, XtrainTotal, YtrainTotal, epoch = 5):\n",
        "    return self.classifier.fit(XtrainTotal, YtrainTotal, epochs=epoch)\n",
        "\n",
        "  def predictModel(self, X_test):\n",
        "    return self.classifier.predict(np.asarray(X_test))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClUVyIzMXre8",
        "colab_type": "code",
        "outputId": "9755f617-a2b3-4288-9479-7fa4d6097173",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        }
      },
      "source": [
        "# # Get Dataset\n",
        "# X_train, Y_train = makeDataset('/content/drive/My Drive/MCA Dataset/train_split_Depression_AVEC2017.csv', 'train_data')\n",
        "# X_dev, Y_dev = makeDataset('/content/drive/My Drive/MCA Dataset/dev_split_Depression_AVEC2017.csv', 'dev_data')\n",
        "# X_test, Y_test = makeDataset('/content/drive/My Drive/MCA Dataset/full_test_split.csv', 'test_data')\n",
        "# YtrainTotal = np.concatenate((Y_train, Y_dev))\n",
        "# XtrainTotal = X_train + X_dev \n",
        "# XtrainTotal = np.asarray(XtrainTotal, dtype=np.float32)\n",
        "\n",
        "# # Run Model On Test Set\n",
        "model = CNN_Video()\n",
        "model.fitModel(XtrainTotal, YtrainTotal, 5)\n",
        "Y_pred = model.predictModel(X_test)\n",
        "print(classification_report(Y_test,Y_pred))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "4/4 [==============================] - 1s 202ms/step - loss: 1068.7135\n",
            "Epoch 2/5\n",
            "4/4 [==============================] - 1s 156ms/step - loss: 0.0000e+00\n",
            "Epoch 3/5\n",
            "4/4 [==============================] - 1s 157ms/step - loss: 0.0000e+00\n",
            "Epoch 4/5\n",
            "4/4 [==============================] - 1s 157ms/step - loss: 0.0000e+00\n",
            "Epoch 5/5\n",
            "4/4 [==============================] - 1s 159ms/step - loss: 0.0000e+00\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00         2\n",
            "\n",
            "    accuracy                           1.00         2\n",
            "   macro avg       1.00      1.00      1.00         2\n",
            "weighted avg       1.00      1.00      1.00         2\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}